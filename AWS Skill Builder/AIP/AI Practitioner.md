## Fundamentals of AI and ML
### Basic AI concepts and terminologies
- **AI**- simulation of human intelligence by machines like computer systems
	- systems that perceive, reason, learn and act based on data performing tasks like humans would with human-intelligence, pattern recognitions, deciison making, problem solving
- **ML**- subset of AI focusing on building systems that learn from data to make predicitons or decisions
	- not being explicilty programmed, but algorithms detecting patterns and relationships in data to improve performance over time
- **Deep Learning**- subset of ML
	- involves neural networks that has many layers (deep)
	- can automatically learn features and representations within large amounts of data
	- common in image recognition, speech processing and natural language understanding
- **Neural Networks**- set of algorithms designed to recognise patterns
	- inspired by the structure of human brain
	- layers of interconnected nodes where each connection carries a weight and data flows through from input to output
	- each node may change the input in a certain wayu and weigh on where the next node will be fired
- **Computer Vision**- field of AI focused on allowingm achines to be able to detect through visual information like images and videos
- **Natural Language Processing (NLP)**- deals with the interaction between human language and machines provessing by understanding, interpreting and generating
- **Model**- used in ML, it has been trained on data to make predictions specifically from the data trained on, does not require being programmed to do the task
- **Algorithm**- takes an input and then a process of logic is performed to create a specific output
- **Training**- feeding data to the ML to be trained on for better pattern recnognition and creating relationships between the data
- **Inference**- process using trained ML models for making predictions on specific tasks ased on new, unseed data
- **Bias**- systematic errors in ML model predicitons due to prejudice assumptions during the learning process
- **Fairness**- ensuring ML models predictions are impartial and do not favor any group over others
- **Fit**- how well the ML models prediction match the actual data. 
- **Overfitting**- occurs when the model learns the data too well and performs poorly on unseen data because it just uses the trained data
- **LLM**- type of deep learning model trained on vast amount of text data to understand human-like-language (NLP)
- **Similarities between AI, ML, DL**-
	- enable machines to perform tasks that usually require human intelligence
	- use data to make decisions
	- using pattern recognition from trained data to make the predictions
- **DIfferences between AI, ML, DL**-
	- AI- broadest concept, encompassing any machine with human-like intelligence
	- ML- subset of AI focussing on algorithms that learn from data to make deicisions
	- DL- using neural networks with multiple layers to model intricate data patterns
- **Types of Inferencing**-
	- **Batch**- processing large number of data inputs at once, suitable for tasks where real-time responses aren't necessary
		- like having large chunks sent in periods
	- **Real-time**- data inputs individually as they arrive, these are immediate predictions or decisions, essential for instant responses
- **Types of Data**-
	- **Labeled**- data comes with tags and labels for annotation, this allows for identifying correct and incorrect output for each input- supervised learning
	- **Unlabeled**- no explicit tags, this is used in unsupervised learning to discover hidden patterns
	- **Tabular**- structured data organised in tables with rows and columns, common in databases and spreadsheets
	- **Time Series**- data points collected or recorded at specific time intervals, used in forecasting and trend analysis
	- **Image**- visual data formats like JPEG/ PNG and used in computer vision tasks
	- **Text**- data in textual format, such as documents, social media posts- used in NLP
	- **Structured**- organised and easily searchable often stored in relational databases
	- **Unstructured**- without predefined format, such as videos, images and free-text documents
- **Types of Learning**-
	- **Supervised**- using labeled data to explicilty tell the ML model what is correct and incorrect- regression (linear like stocks) or classification (categorical)
	- **Unsupervised**- uses unlaeled data to indeitfy patterns between the input and create clustering and associated tatsks (groups common inputs together to identify some sort of common pattern)
	- **Reinforcement**- agent learns to make decisions by performing actions in an environment to maximize cumulative rewards, used in robotics and game playing

### Identify practical use cases for AI
- **AI Is appropriate use case**- can enhance decisions making, automatig repetitive tasks, scaling solutions efficiently
	- **Assist human decision making**- 
		- **Healthcare**- ML models can analyse medical images (x-rays, MRI) to assist doctors in diagnosing conditions with higher accuracy
		- **Finance**- analyse stock market trends, help financial analysts make more informaed investment decisions
		- **Custtomer support**- NLP powered chatbots can resolve common customer issues, leaving human agents for more complext cases
	- **Solution Scalability**- 
		- **Personlised marketing**- based on tracking human behaviour, can tagert individuals more specifically based on their browsing
		- **Supply chain optimization**- forecasting models can predict demand, optimize inventory and reduce waste
		- **Content moderation**- automatically filter inappropriate content on social media platforms, scaling moderation to millions of users
	- **Automation**-
		- **Manufactoring**- robots can utomate quality control and assembly processes
		- **Smart home automation**- optimize energy consumption by learning usage patterns and controlling devices accordingly
		- **Fraud detection**- real-time ML models can flag suspicious transactions for financial institutions
- **AI is not appropriate use case**- AI/ML not always best approach, can use traditional programming or rule-based systems
	- **Cost-benefit analysis**
		- **Small-scale projects**- cost of implementing ML slutions can outweigh the benefits if volume of data is insufficient or decisions are straightforward
		- **Limited data available**- not enough historical data to train an effective model, rule based approach might be more efficient
		- **Long development time**- if project requires immediate implementation, developing ML model too time consuming
	- **When specific outcome is needed**
		- If task requires precise, deterministic output rather than probabilistic prediction, ML not suitable. eg accounting, legal contract generation where accuracy is critical, deterministic algoprithms are preferable
- **Selecting appropriate ML techniques for specific use cases**
	- **Regression**-
		- **Use**- predicting continous outcomes- stock prices, temperature forecases, house prices
		- **AWS**- Sagemaker can build, train and deploy regression models using bult-in algorithms
	- **Classification**-
		- **Use**- categorical data stored in distinct classes such as spam detection, disease diagnosis or sentimen analysis
		- **AWS**- Sagemaker built-in algorithms like XGBoost can be used for classification tasks
	- **Clustering**-
		- **Use**- grouping similar items together, such as customer segmentation, market analysis or document clustering
		- **AWS**- Sagemaer supports clustering algorithms like K-means
- **Identify examples of real world AI applications**
	- **Computer Vision**-
		- **Use**- autonomous vehicles, facial recognition for security, quality control in manufaction, agriculture food sorting
		- **AWS**- Rekognition for image and video analysis
	- **Natural Language Processing**-
		- **Use**- customer support chatbots, sentiment analysis, document summarisation
		- **AWS**- Comprehend for extracting insights from text data
	- **Speech Recognition**-
		- **Use**- Transcribing customer support calls, voice-activated assistants (Alexa)
		- **AWS**- Transcribe for converting speech to text
	- **Recommendation Systems**-
		- **Use**- E-commerce product recommendations, movie suggestions on streaming platforms
		- **AWS**- Personalize for real-time personalised recommendations
	- **Fraud Detection**-
		- **Use**- Identifying fradulent credit card transactions, preventing identity theft
		- **AWS**- Sagemaker anomoly detection models
	- **Forecasting**-
		- **Use**- Demand forecasting, inventory management, resource planning
		- **AWS**- Forecast for time-series forecasting
- **Capabilities for AWS ML Services**
	- **Sagemaker**- fully managed service that helps data scientists and devs build, train, deploy ML models at scale. Supports variety of ML algorithms, automated model turning and model deployment
	- **Transcribe**- Automated speech recognition service converst speech into text. Useful for transcription of audio files, meeting recordings or real time speech
	- **Translate**- neural machine translation ervice that provides fast and accurate language translation, ideal for translating customer communications or documents
	- **Comprehend**- NLP service uses ML to extract key phrases, entities, sentiment and language from unstructured text. useful for customer feedback, social media monitoring and document classification
	- **Lex**- building conversational interfaces using voice and text. Powers chatbots and virtual assistants with automatic speech recognition and NLP
	- **Polly**- Text to speech service that converts written text into lifelike spoeech. Ideal for applications requiring voice interaction, like virtual assistants or audio content generation
### Describe ML Development Lifecycle
- The development lifecycle of how machine learning pipeline works
	1. **Data Collection**-
		- Purpose- gather data from various sources like databases, APIs, IoT devices to use for training
		- AWS- S3 for dat astorage, Glue for data extraction and transformation
	2. **Exploratory Data Analysis (EDA)**-
		- Purpose- understand the data by visualising patterns, outliers and data imbalances, make assumptions of the data
		- AWS- Sagemaker Data Wrangler for exploratory data analysis and data visualisation, QuickSight for business intelligence tool
	3. **Data Pre-Processing**-
		- Purpose- Clean and prepare the data, including handling missing values, normalization, and transofrmation- preparing the data to be trained, data needs to be to a certain standard for training to work properly
		- AWS- Sagemaker Processing for batch data pre-processing and Glue for multiple data sources connections
	4. **Feature Engineering**-
		- Purpose- Create new features or modify existing ones to improve the model performance
		- AWS- Sagemaker Feature Store for managing and storing features
	5. **Model Training**- 
		- Purpose- train ML models on labeeled data
		- AWS- Sagemaker Training Jobs for distributed and efficient training, Deep Learning AMIs
	6. **Hyperparameter Tuning**-
		- Purpose- Optimize model hyperparameters to improve performance
		- AWS- Sagemaker Automatic Model Tuning for hyperparameter optimization
	7. **Model Evaluation**-
		- Purpose- Assess model performance using metrics like accuracy, precision, recall and AUC
		- AWS- Sagemaker Experiments to track evaluation metrics and compare results, Cloudwatch Logs for tracking of performance
	8. **Deployment**-
		- Purpose- Deplo trained model for real-time or batch inference
		- AWS- Sagemaker Endpoint for real-time deployment, AWS Lamdbda for lightweight deployment
	9. **Monitoring**-
		- Purpose- Monitor the deployed model for drift, performance degradation or errors and make adjustments- loop through again
		- AWS- Sagemaker Model Monitor for automated monitoring, Cloudwatch Metrics for evaluation of stats
- **Sources of Models**- 
	-  **Open-Source Pre-Trained Models**
		- Hugging Face Transformers, Tensorflow Hub, PyTorch Hub
		- Reduces development time by leverageing state of the art models by large companies, ready to go code with higher abstraction
	- **Training Custom Models**
		- Created from scratch using specific data tailored to business problem
		- Sagemaker offers support for custom model training using tensorflow, pyorch and MXNet
- **Methods to Use a Model in Production**-
	- **Managed API Service**-
		- Deploy model using services like Sagemaker Endpoints, which handle scaling and availability
		- AWS- API Gateway + SageMaker Endpoint
	- **Self-Hosted API**-
		- Host model on custom servers or containers using Docker and Kubernetes
		- AWS- EC2, EKS
- **Fundamental Concepts of MLOps**-
	- **Experimentation**-
		- Iterative model development using different datasets, algorithms and hyperparameters- to see what sort of matching variables work best
		- AWS- Sagemaker Experiments to track and manage experiments
	- **Repeatable Processes**-
		- Automate ML pipelines for consistend and reproducible results- be able to consistently output the same method with automative processes
		- AWS- Step Functions + Sagemaker Pipelines
	- **Scalable Systems**-
		- Ensure ML solutions scale with increasing data and users- using things like containers, auto balancers, load balancers for scalability
		- AWS- Sagemaker Training with managed spot instances for scalability
	- **Managing Technical Debt**-
		- Minimise complexity by maintaining clear and modular piplelines- ensuring it doesnt get too complicated to manage and deply the ML pipline
	- **Production Readiness**-
		- Validate models before deployment using robust testing frameworks- ensuring that the model being used has based a baseline minimum so its not trash
	- **Model Monitoring and Re-Training**-
		- Continously monitor model performance and retrain using updated data- after identifying issues 
		- AWS- Sagemaker Model Monitor + Sagemaker Pipinelines for retraining
- **Model Performance Metrics**-
	- **Model Metrics**- 
		- **Accuracy**- percentage of correct predictions out of total predictions
		- **Precision**- ratio of true positives to all predicted positives
		- **Recall**- ratio of true positive to all actual positives
		- **F1 Score**- harmonic mean of precisions and recall, balancing both
		- **Area Under the ROC Curve (AUC)**- measure the models ability to distinguish between classes
	- **Business Metrics**-
		- **Cost per User**- operational cost divided by the number of users benefiting from the model
		- **Development Costs**- resources (time, infrastructure) spent on model development
		- **Customer Feedback**- direct feedbacj from end-users on the models usefulness and accuracy
		- **Return on INvestment (ROI)**- value derived from the model compared to its cost of development and maintenance
- **AWS Managed AI/ML Services for ML Lifecycle**-
	- **Sagemaker**- end-to-end ML development, including training, deployment and monitoring
	- **Sagemaker Data Wrangler**- simplified data preparation and exploration
	- **Sagemaker Feature Store**- centralised repository for storing and retrieving ML features
	- **Sagemaker Model Monitor**- automatic monitoring of deployed models for data drift and errors
	- **Sagemaker Pipelines**- automate and manage ML workflows

## Fundamentals of Gen AI
### Basic Concepts of Gen AI
- **Foundational Gen AI Concepts**-
	- **Tokens**- 
		- Smalles unit of text or data that a model processes, LLM tokens could represent words, characters or subwords
		- Example- AI is fun tokenise as ["AI", "is", "fun"]- this depends on how the tokeniser does it, as it could be split into singular characters as well
	- **Chunking**- 
		- Dviding data into manageable chunks for efficient processing, like splitting a long document into smaller sections before feeding it to the model
		- usefule in tasks like summarisation or document processing
	- **Embeddings**-
		- Dense vector representation of data (text, images) where similar items are closer in a vector space
		- Example- Word2Vec creates embeddings where synonyms like "king" and "queen" are geometrically close- like categorising words that are related in a way together in a 3D space (vectors)
	- **Vectors**- 
		- Numeric representations of data (embeddings), vectors enable computational efficiency in similarity search or clustering tasks- the vectors are stored like etc like nested coordinates on a 3D plane
	- **Prompt Engineering**-
		- crafting specific intructions to prompt to the model to guide the gen AIs behaviour
	- **Transformer-Based LLM**- 
		- LLM built on transformer architecture use self-attention mechanisms to understand context across entire sequences- the ability to understand and remember better
		- good at tasks like translation, summarisation and text generation
	- **FOundation Models**-
		- large pre-trained models capable of being fine-tuned for a variety of downstream tasks, they form the foundation for many AI applications
		- OpenAI GPT models, hugging face models, AWS Bedrock models
	- **Multi-modal Models**-
		- Models designed to process and generate data across multiple modalities (text, image, video)
		- DALL-E generates images from text
	- **Diffusion Models**-
		- generative model that creates data by iteraveily removing noise, used for image generation and restoration
		- Stable diffusion, DALL-E
- **Use Cases for Gen AI**-
	- **Image Generation**-
		- DALL-E and stable diffusion use text-to-image prompting
		- Use- marketing, social media, digital art
	- **Video Generation**-
		- Generate animated or syntheised videos based on text descriptions or templates
		- Use- virtual characters, movie effects
	- **Audio Generation**-
		- Text-to-soeech and AI generated music
		- Use- accessibility with like reading out to blind people
	- **Summarisation**-
		- Condensing long documents into concise summaries using LLMs
		- Use- news aggregation, legal document analysis
	- **Chatbots**-
		- Conversational agents poweredby LLMs 
		- Use- customer support, virtual assistants
	- **Translation**-
		- Gen models like AWS Trranslate provide context-aware language translation
		- Use- cross-language translation, content localisation
	- **Code Generation**-
		- Github Copilot assist developers by generating boilerplate code or solving problems
		- Use- software development, debugging
	- **Customer Service Agents**-
		- Agents handle customer queries with human-like responses
		- Use- e-commerce, banking
	- **Search**-
		- Context-aware search engines provide accurate results using embeddings and LLMs
		- Use- knwoeldge managementm, document search
	- **Recommendation Engines**-
		- Personalised reocmmendations using user behasviour and embeddings
		- Use- e-commenrce, streaming platforms
- **Foundation Model Lifecycle**-
	1. **Data Selection**-
		- identify diverse, high-quality datasets for pre-training
		- Example- text data from books, websites and research articles
	2. **Model Selection**-
		- Choose architecture best suited for the problem (eg GPT for test, diffusion for images)
		- Example- GPT for NLP tasks
	3. **Pre-Training**-
		- Train the model on a vast corpus of data to learn generalised representations
		- AWS- Sagemaker supports large-scale distribured training
	4. **Fine-Tuning**-
		- Adapt the model to specific use cases or industries using domain-specific datasets
		- AWS- Sagemaker fine-tuning for customising foundation models
	5. **Evaluation**-
		- Assess models performance using metrics like BLEU (text) FID (images)
		- AWS- Sagemaker Experiments to track evaluation results
	6. **Deployment**-
		- Make model available via APIs or endpoints for real-time batch inference
		- AWS- Sagemaker Endpoint for managed deployment
	7. **Feedback**-
		- Gather user feedback to idetnfiy areas of improvement
		- Example- continous monitoring of chatbot interactions
	8. **Model Iteration**-
		- Retrain or fine-tune model periodically to incorporate feedback and updated data
		- AWS- Sagemaker Pipelines for automating retraining workflows
- **AWS Gen AI Services**-
	- **Sagemaker**- end-to-end development and deployment of generative models
	- **Polly**- text-to-speech for audio generation
	- **Translate**- language translation services
	- **Rekognition**- image and video analysis for visual AI tasks
	- **Lambda**- lightweight deployment of gen AI models for scalable APIs