- **Machine learning types**-
	- **Supervised learning**- all data is labelled and alrogirhtms learn to predict the output from the input data (classification and regression)
	- **Unsupervised learning**- all data is unlabelled and the algorithms learn to inherent structure from the input data (clustering)
	- **Semi0supervised learning**- some data is labelled, but most is unlabelled, mixture of supervised and unsupervised techniques used (pattern recognition (mix of labelled and unlabelled images))
- **Types of Learning**-
	- **Inductive Learning**- learning from example, e.g. learning concept of 'tiger' by being given pictures of 10 different tigers (and being told the animals are tigers)
	- **Learning by being told**- e.g. telling someone 'a tiger has a long tail, stripes, short ears etc'
	- **Learning by anology**- e.g 'a tiger is like a big cat with orange stripes and big teeth'- related to learning by discovery
- **Classification**- many learning problems can be expressed as classification problems. Given varios input features (like size of teeth, stipiness) the goal is to decide which of several classes (or output categories) the case falls into (e.g tiger, rabbit, giraffe etc). 
	- Applications of classification include:
		- **Character recognition**
		- **Disease diagnosis**
		- **Credit screening**
	- Goal of classification problems is to find some rule or function that lets you classify an instance into a particular class based on the features of that isntance
		- f(feature1, feature2, ...) Class
		- fteeth=big, covering=stripy
	- try to discover the function *f* from a data set of cases which we know the correct classification
- **Terminology**-
	- ![[Pasted image 20240831161840.png]]
	- **Sample (example)**- set of data that describes an entity. In a table, each row represents a sample
	- **Attribute (feature)**- refers to a charactersitic of samples. in a table, each column represents an attribute
	- **Feature Value**- values in the main body of the table are the attribute (feature) values
	- First column in table would not be considered an attribute
		- because its a unique identifier, so not considered a predicator attribute ( an attribute used to make classification decision)
	- In a table if a row (sample) is equal to what you are looking for, then it is positive sample ,if its not what looking for then its negative sample
- **Data Types**- 
	- **Boolean**- usually 'yes' or 'no' (true or false)
	- **Catergorical**- taking discrete value (like boolean)
		- **Nominal (categorical)**- have two or more discrete values (ege red, blue, green)
		- **Ordinal (categorical)**- two or more discrete values, but there is an implicit ordering of these values (young, middle aged, old)
	- **Numeric**- a number
		- **Interval (numeric)**- measured on an interval scale (temperature measured on Celsius scale where each degree is 1/1000th the difference between melting and boiling points water)
		- **Ratio (numeric)**- value is ratio between magnitude of a quantity and a unit of magnitude of same kind (mass, length, time)
	- **Structured data types**- include categorical and numeric values
	- **Unstructured data types**- include textual and multimedia values
	- **Semi-structured values**- include webpages, email and html/xml data
- **KNowledge representation**-
	- before learning, must decide how going to represent knowledge
	- exmaple, suppose we represent knowledge as IF, THEN tules in left hand side is a conjunction of features
		- e.g. (stripy = yes) ^ (long tail = yes) -> (tiger? = yes)
- **Searvhing rule space**- search for possible rules
	- inductive learning- finding clever ways of managing search for possible rules
- **Decision Trees**- useful structure for classification
	- **Classification using Decision Tree**- 
		- decision tree allows to devide how to classify an example as animal, based on attribute values
		- basically a tree based on logic, branches of YES/NO which go to more questions
		- **Non-Leaf NOdes** represent attributes (striped, long tail, big teeth etc)
		- **LEaf NOdes** represent classes (tiger, lemur, zebra, rabbit etc)
		- **Arc Labels** represent attribute values (yes, no)
		- traverse the tree based on the attribute values to get to the class answer
	- **DEcision tree and Conjunctive Rules**-
		- a decision tree really just stores set of c onjuctive rules (1 rule for each leaf node)
		- (stripy = Yes) ^ (long tail = Yes) ^ (big teeth = yes) -> (class = tiger)
	- **DEcision tree inductio**- 
		- **OKhams Razor**- states that the most likely hypothesis is the simplest one that is consister with all observations
		- the smallest tree that correctly classifies all of the training examples is best
		- induction algorithms aim to come up with the ssimplest decision tree that coers the exampl data
		- prefer the simplest tree becasue we bleieve that it is more likely to be a general rule which works for new cases, rather than a complex forumala applied to given data
		- resulting tree is a useful reprepentation that can be easily verified by humans and used by humans or pgraomgas
		- cerate decision tree top down looking for fwatures with which to lavel nodes
		- feature chosen shoul be a good discriminating feature- should be the feature which best splits the exampls into different classes (tifer, non-tiger)
			- IMPORTANT- starting with an attribute at the root comapred to others can cause the traversing to take longer 
			- find the best discriminating feature (which will be best at root to get rid of any NON classes for what you are looking for)
				- highest amount of information that we can use to start getting rid of NON classes
- **ID3 Algorithm**- 
	- decision tree induction program constructs decision trees in top-down
	- attribute seleted to test current node of the tree and is used to partition set of examples
	- algorithm recusrively constructs a sub-tree for each partition
	- continues until either all memebers of partition are in same class or no mroe attribute remains
	- **Criteria**- select criteria o be in root
		- selecting attribute to be place of root of tree (and each of sub trees) is critical for inducing a good tree
		- criteria used by ID3 based on measure called **information again**
	- **Entropy**- 
		- set of samples and each sample belongs to either a class N or P
		- probability of the sample belonging to class P or N
	- **Information Gain**- simple expected reducton in entropy casued by partitioning examples acoording to some attribute
		- more information gathered from attributes, we can reduce the possibilities that a sample will be a part of
		- ![[Pasted image 20240902191005.png]]
		- when the attribute has the greatest information gain, ID3 will select it as the root of the tree
		- algorithm will then continue to apply this alysis recursively to each sub-tree until has completed the tree
		- root attribute can already identify sample that isnt part of the class so can remove those rows from the table immediately
	- **Measuring Classification Accuracy**-
		- real test of how well a classifier performs must be determined by testing its perfoamce on samples thath ave not been used to construct the classifier
		- no point in having classifier that performs well on training samples if it doesnt perform well in predicting class of test samples
		- classifier must be able to generalise
		- test ability of classifier to generalise we must hold out some of the samples to use as test set
		- **Estimate Methodologies**-
			- **SImple Split**- 
				- split data into two mutually exclusive sets (training set- 70%, and test set 30%)
				- include decision tree using examples in training set, and then measure classification accuracy on examples in test set
			- **K-fold Cross Validation**
				- split the data into k mutually exclusive subsets
				- use each subset as a test set, while using all of other subsets as for training
				- repeat step 2 k times
				- aggregate the test results to obtain the overall estimation of prediciton accuracy
			- **Leave-one-out Cross Validation**
				- special case of k-fold where k is equal to number of examples in the dataset
				- use one sample for testing
			- which one to use-
				- no definitive answer but really matters of tradeoff
				- simple split is good for large number of examples, if its extremely large might do a 10% training and 90% test split, if number is small then number of examples used for training may not be sufficient to develop good model
				- k-fold is good for estimation of true accuracy of classifier, but is computaitonally expenseive, since need to inudec many decision rees, best for small datasets and moderate data set size
	- **Overfitting**-
		- good performance is seen on training data, but poor erfroamcen on test data
		- overfits to the training data- good for training but not test
	- **Generalisation**-
		- dont care how well classifier performs on examples it has trained on. Interested in how well it performs on examples it hasnt seen before, interested in its ability to generalise
		- higher degree of overfitting, lower ability of classifier to generalise
	- **Pruning**-
		- avoding overfitting is using pruning in decision tree
		- cutting off smaller branches
		- result in new tree that does not eprform well on training examples but better able to generalise corretly predict class of novel examples
		- different approaches to pruning
		- no in ID3 algorithm